{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT - Depth Growing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Depth Growing For Neural Machine Translation  \n",
        "\n",
        "Tensorflow implementation of the paper https://arxiv.org/abs/1907.01968  \n",
        "The implementation is based on tensorflow tutorial on NMT https://www.tensorflow.org/tutorials/text/nmt_with_attention  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "colab_type": "code",
        "id": "WTJgQhWP6-K4",
        "outputId": "5144634b-c3bd-4b85-c9a8-40ca317a3768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/26/08/58267cb3ac00f5f895457777ed9e0d106dbb5e6388fa7923d8663b04b849/subword_nmt-0.3.6-py2.py3-none-any.whl\n",
            "Installing collected packages: subword-nmt\n",
            "Successfully installed subword-nmt-0.3.6\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install subword-nmt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xueHHN7PCm5i"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Y3Ce25eUGLrg"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow tensorflow-datasets matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TgNfkbrWGRCt"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cw1_E6pWh4sm"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from tensorflow.keras import layers as layers\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_nufW0fMsPFm"
      },
      "source": [
        "**Implementation of the Transformer module**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "![Transformer](https://camo.githubusercontent.com/4b80977ac0757d1d18eb7be4d0238e92673bfaba/68747470733a2f2f6c696c69616e77656e672e6769746875622e696f2f6c696c2d6c6f672f6173736574732f696d616765732f7472616e73666f726d65722e706e67)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-jl62-nYjQhl"
      },
      "outputs": [],
      "source": [
        "def scaledDotAttentionWrapper(v, k, q, mask=None):\n",
        "  matMul = tf.matmul(q, k, transpose_b=True)\n",
        "  scaled = matMul / K.sqrt(tf.cast(tf.shape(k)[-1], tf.float32)) # tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  masked = scaled\n",
        "  if mask is not None:\n",
        "    masked = scaled + (mask * -1e9)\n",
        "  softmax = K.softmax(masked)\n",
        "  out = tf.matmul(softmax, v)\n",
        "  return out, softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "io5QsVB1lOnE"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = K.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = K.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = K.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "temp_out, temp_attn = scaledDotAttentionWrapper(\n",
        "      temp_v, temp_k, temp_q)\n",
        "print ('Attention weights are:')\n",
        "print (K.eval(temp_attn))\n",
        "print ('Output is:')\n",
        "print (temp_out)\n",
        "print (K.eval(temp_out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "b4naEhK7mO6E"
      },
      "outputs": [],
      "source": [
        "def multiHeadAttentionWrapper(v, k, q, mask=None, d_model=512, num_heads=8):\n",
        "  depth = d_model // num_heads\n",
        "  \n",
        "  def split_heads(x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, num_heads, depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "  \n",
        "  batch_size = tf.shape(q)[0]\n",
        "  \n",
        "  wv_splitted = split_heads(layers.Dense(d_model)(v), batch_size)\n",
        "  wk_splitted = split_heads(layers.Dense(d_model)(k), batch_size)\n",
        "  wq_splitted = split_heads(layers.Dense(d_model)(q), batch_size)\n",
        "  \n",
        "  scaled_attention, att_weights = scaledDotAttentionWrapper(wv_splitted, wk_splitted, wq_splitted)\n",
        "  scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "  concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, d_model))\n",
        "  out = layers.Dense(d_model)(concat_attention)\n",
        "  return out, att_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_o03vEedEnTt"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "  def __init__(self, d_model=512, num_heads=8):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.depth = d_model // num_heads\n",
        "    self.num_heads= num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "  \n",
        "  def call(self, v, k, q, mask=None):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    w = layers.Dense(self.d_model)\n",
        "    wv_splitted = self.split_heads(w(v), batch_size)\n",
        "    wk_splitted = self.split_heads(w(k), batch_size)\n",
        "    wq_splitted = self.split_heads(w(q), batch_size)\n",
        "    att_out, att_weights = scaledDotAttentionWrapper(wv_splitted, wk_splitted, wq_splitted, mask=mask)\n",
        "    att_out = tf.transpose(att_out, perm=[0, 2, 1, 3])\n",
        "    att_out = tf.reshape(att_out, (batch_size, -1, self.d_model))\n",
        "    att_out = w(att_out)\n",
        "    return att_out, att_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "f9UjO8ZgteFs"
      },
      "outputs": [],
      "source": [
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = multiHeadAttentionWrapper(y, k=y, q=y, mask=None, d_model=512, num_heads=8)\n",
        "out.shape, attn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9JgJxnswH7_2"
      },
      "outputs": [],
      "source": [
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = MultiHeadAttention()(y, y, y)\n",
        "out.shape, attn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sJigm2NovifN"
      },
      "outputs": [],
      "source": [
        "def encoder_layer(X, training, d_model=512, num_heads=8, mask=None, dff=2048 ,dropout_rate=0.1):\n",
        "  attention_out, _ = multiHeadAttentionWrapper(X, k=X, q=X, mask=mask, d_model=d_model, num_heads=num_heads)\n",
        "  attention_out = layers.Dropout(dropout_rate)(attention_out, training)\n",
        "  X = layers.LayerNormalization(epsilon=1e-6)(attention_out + X)\n",
        "  feed_forward = layers.Dense(dff, activation='relu')(X)\n",
        "  feed_forward = layers.Dense(d_model)(feed_forward)\n",
        "  feed_forward = layers.Dropout(dropout_rate)(feed_forward, training)\n",
        "  X = layers.LayerNormalization(epsilon=1e-6)(X + feed_forward)\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pR9X6x6VTR9D"
      },
      "outputs": [],
      "source": [
        "sample_encoder_layer_output = encoder_layer(tf.random.uniform((64, 43, 512)), False)\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aSkAsSgPJTVn"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "  def __init__(self, d_model=512, num_heads=8, dff=2048 ,dropout_rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.dm = d_model\n",
        "    self.nh = num_heads\n",
        "    self.dff = dff\n",
        "    self.dr = dropout_rate\n",
        "    \n",
        "  def call(self, X, training, mask=None):\n",
        "    out, _ = MultiHeadAttention(self.dm, self.nh)(X, X, X, mask)\n",
        "    out = layers.Dropout(self.dr)(out, training)\n",
        "    out = layers.LayerNormalization(epsilon=1e-6)(out + X)\n",
        "    feed_forward = layers.Dense(self.dff, activation='relu')(out)\n",
        "    feed_forward = layers.Dense(self.dm)(feed_forward)\n",
        "    feed_forward = layers.Dropout(self.dr)(feed_forward, training)\n",
        "    out = layers.LayerNormalization(epsilon=1e-6)(out + feed_forward)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3GYohy21L-gi"
      },
      "outputs": [],
      "source": [
        "sample_encoder_layer_output = EncoderLayer()(tf.random.uniform((64, 43, 512)), False)\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EQ6Bj9myU6TN"
      },
      "outputs": [],
      "source": [
        "def decoder_layer(X, enc_output, training, d_model=512, num_heads=8, look_ahead_mask=None, padding_mask=None, dff=2048, dropout_rate=0.1):\n",
        "  attention_out, att_w1 = multiHeadAttentionWrapper(X, k=X, q=X, mask=look_ahead_mask, d_model=d_model, num_heads=num_heads)\n",
        "  attention_out = layers.Dropout(dropout_rate)(attention_out, training)\n",
        "  X = layers.LayerNormalization(epsilon=1e-6)(attention_out + X)\n",
        "  attention_out, att_w2 = multiHeadAttentionWrapper(enc_output, k=enc_output, q=X, mask=padding_mask, d_model=d_model, num_heads=num_heads)\n",
        "  attention_out = layers.Dropout(dropout_rate)(attention_out, training)\n",
        "  X = layers.LayerNormalization(epsilon=1e-6)(attention_out + X)\n",
        "  feed_forward = layers.Dense(dff, activation='relu')(X)\n",
        "  feed_forward = layers.Dense(d_model)(feed_forward)\n",
        "  feed_forward = layers.Dropout(dropout_rate)(feed_forward, training)\n",
        "  X = layers.LayerNormalization(epsilon=1e-6)(X + feed_forward)\n",
        "  return X, att_w1, att_w2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8HevJ2z8W_6r"
      },
      "outputs": [],
      "source": [
        "sample_decoder_layer_output, _ , _ = decoder_layer(tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, False)\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sv00fUERMo3H"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "  def __init__(self, d_model=512, num_heads=8, dff=2048 ,dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.dm = d_model\n",
        "    self.nh = num_heads\n",
        "    self.dff = dff\n",
        "    self.dr = dropout_rate\n",
        "    \n",
        "  def call(self, X, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "    mha1, att_w1 = MultiHeadAttention(self.dm, self.nh)(X, X, X, look_ahead_mask)\n",
        "    mha1 = layers.Dropout(self.dr)(mha1, training)\n",
        "    out = layers.LayerNormalization(epsilon=1e-6)(mha1 + X)\n",
        "    mha2, att_w2 = MultiHeadAttention(self.dm, self.nh)(enc_output, enc_output, out, padding_mask)\n",
        "    mha2 = layers.Dropout(self.dr)(mha2, training)\n",
        "    out = layers.LayerNormalization(epsilon=1e-6)(out + mha2)\n",
        "    feed_forward = layers.Dense(self.dff, activation='relu')(out)\n",
        "    feed_forward = layers.Dense(self.dm)(feed_forward)\n",
        "    feed_forward = layers.Dropout(self.dr)(feed_forward, training)\n",
        "    out = layers.LayerNormalization(epsilon=1e-6)(out + feed_forward)\n",
        "    return out, att_w1, att_w2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2DFaKsLWNkLl"
      },
      "outputs": [],
      "source": [
        "sample_decoder_layer_output, _ , _ = DecoderLayer()(tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, False)\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xdbTN7mSXgje"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(max_position, d_model):\n",
        "  pos_encodings = np.zeros((max_position, d_model))\n",
        "  angles = np.arange(max_position)[:, np.newaxis] / np.power(1000, np.arange(d_model)[np.newaxis, :] * 2 / np.float32(d_model))\n",
        "  pos_encodings[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "  pos_encodings[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "  pos_encodings = pos_encodings[np.newaxis, ...]\n",
        "    \n",
        "  return (pos_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2G0ZUVXjNKj3"
      },
      "outputs": [],
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H8EudKVDNPML"
      },
      "outputs": [],
      "source": [
        "def encoder(X, input_vocab_size, maximum_position_encoding, training, num_layers=6, d_model=512, num_heads=8, dff=2048, mask=None ,dropout_rate=0.1):\n",
        "  seq_len = tf.shape(X)[1]\n",
        "  X = layers.Embedding(input_vocab_size, d_model)(X)\n",
        "  pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "  X *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  X += tf.slice(tf.cast(pos_encoding, tf.float32), [0, 0, 0], [pos_encoding.shape[0], seq_len, pos_encoding.shape[2]])\n",
        "  X = layers.Dropout(dropout_rate)(X, training)\n",
        "  for i in range(num_layers):\n",
        "    X = encoder_layer(X, training, d_model, num_heads, mask, dff ,dropout_rate)\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zqEhYPGeRR8p"
      },
      "outputs": [],
      "source": [
        "sample_encoder_output = encoder(tf.random.uniform((64, 62)), 8500, maximum_position_encoding=10000, training=False)\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "J-DhiX4mOEQo"
      },
      "outputs": [],
      "source": [
        "class Encoder(layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_vocab_size: int,\n",
        "      maximum_position_encoding: int,\n",
        "      num_layers: int = 6,\n",
        "      d_model: int = 512,\n",
        "      num_heads: int = 8,\n",
        "      dff: int = 2048,\n",
        "      dropout_rate: int = 0.1\n",
        "  ):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.maximum_position_encoding = maximum_position_encoding\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "  \n",
        "  def call(self, X, training, mask=None):\n",
        "    seq_len = tf.shape(X)[1]\n",
        "    X = layers.Embedding(self.input_vocab_size, self.d_model)(X)\n",
        "    pos_encoding = positional_encoding(self.maximum_position_encoding, self.d_model)\n",
        "    X *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    X += tf.slice(tf.cast(pos_encoding, tf.float32), [0, 0, 0], [pos_encoding.shape[0], seq_len, pos_encoding.shape[2]])\n",
        "    X = layers.Dropout(self.dropout_rate)(X, training)\n",
        "    for i in range(self.num_layers):\n",
        "      X = EncoderLayer(self.d_model, self.num_heads, self.dff , self.dropout_rate)(X, training, mask)\n",
        "    return X\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gCAUdvOEROEC"
      },
      "outputs": [],
      "source": [
        "sample_encoder_output = Encoder(8500, maximum_position_encoding=10000)(tf.random.uniform((64, 62)), training=False)\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QcKAkpfORksl"
      },
      "outputs": [],
      "source": [
        "def decoder(X, enc_output, target_vocab_size, maximum_position_encoding, training, num_layers=6, d_model=512, num_heads=8, dff=2048, look_ahead_mask=None, padding_mask=None, dropout_rate=0.1):\n",
        "  seq_len = tf.shape(X)[1]\n",
        "  X = layers.Embedding(target_vocab_size, d_model)(X)\n",
        "  pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "  X *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  X += tf.slice(tf.cast(pos_encoding, tf.float32), [0, 0, 0], [pos_encoding.shape[0], seq_len, pos_encoding.shape[2]])\n",
        "  X = layers.Dropout(dropout_rate)(X, training)\n",
        "  attention_weights = {}\n",
        "  for i in range(num_layers):\n",
        "    X, block1, block2 = decoder_layer(X, enc_output, training, d_model, num_heads, look_ahead_mask, padding_mask, dff, dropout_rate)\n",
        "    attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "    attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "  return X, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OKWZk1GyUuG_"
      },
      "outputs": [],
      "source": [
        "output, attn = decoder(tf.random.uniform((64, 26)), enc_output=sample_encoder_output, training=False, target_vocab_size=8000, maximum_position_encoding=5000)\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1bhmStjhRdES"
      },
      "outputs": [],
      "source": [
        "class Decoder(layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      target_vocab_size: int,\n",
        "      maximum_position_encoding: int,\n",
        "      num_layers: int = 6,\n",
        "      d_model: int = 512,\n",
        "      num_heads: int = 8,\n",
        "      dff: int = 2048,\n",
        "      dropout_rate: int = 0.1\n",
        "  ):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.target_vocab_size = target_vocab_size\n",
        "    self.maximum_position_encoding = maximum_position_encoding\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "  \n",
        "  def call(self, X, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "    seq_len = tf.shape(X)[1]\n",
        "    X = layers.Embedding(self.target_vocab_size, self.d_model)(X)\n",
        "    pos_encoding = positional_encoding(self.maximum_position_encoding, self.d_model)\n",
        "    X *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    X += tf.slice(tf.cast(pos_encoding, tf.float32), [0, 0, 0], [pos_encoding.shape[0], seq_len, pos_encoding.shape[2]])\n",
        "    X = layers.Dropout(self.dropout_rate)(X, training)\n",
        "    attention_weights = {}\n",
        "    for i in range(self.num_layers):\n",
        "      X, block1, block2 = DecoderLayer(self.d_model, self.num_heads, self.dff, self.dropout_rate)(X, enc_output, training, look_ahead_mask, padding_mask)\n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    return X, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VXBkYx4ISUnP"
      },
      "outputs": [],
      "source": [
        "output, attn = Decoder(target_vocab_size=8000, maximum_position_encoding=5000)(tf.random.uniform((64, 26)), enc_output=sample_encoder_output, training=False)\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qA4BhvqOU2kX"
      },
      "outputs": [],
      "source": [
        "def transformer(\n",
        "    inp, tar, input_vocab_size, target_vocab_size, pe_input, pe_target, training,\n",
        "    num_layers=6, d_model=512, num_heads=8, dff=2048, dropout_rate=0.1, enc_mask=None, look_ahead_mask=None, padding_mask=None):\n",
        "  enc_output = encoder(inp, input_vocab_size, pe_input, training, num_layers, d_model, num_heads, dff, enc_mask ,dropout_rate)\n",
        "  dec_output, attention_weights = decoder(tar, enc_output, target_vocab_size, pe_target, training, num_layers, d_model, num_heads, dff, look_ahead_mask, padding_mask, dropout_rate)\n",
        "  final_output = layers.Dense(target_vocab_size)(dec_output)\n",
        "  return final_output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lxw3jLm9SyZI"
      },
      "outputs": [],
      "source": [
        "class Transformer(layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_vocab_size: int,\n",
        "      target_vocab_size: int,\n",
        "      pe_input: int,\n",
        "      pe_target: int,\n",
        "      num_layers: int=6,\n",
        "      d_model: int=512,\n",
        "      num_heads: int=8,\n",
        "      dff: int=2048,\n",
        "      dropout_rate: int=0.1\n",
        "  ):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.target_vocab_size = target_vocab_size\n",
        "    self.pe_input = pe_input\n",
        "    self.pe_target = pe_target\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "  \n",
        "  def call(self, inp, tar, training, enc_mask=None, look_ahead_mask=None, padding_mask=None):\n",
        "    enc_output = Encoder(self.input_vocab_size, self.pe_input, self.num_layers, self.d_model, self.num_heads, self.dff, self.dropout_rate)(inp, training, enc_mask)\n",
        "    dec_output, attention_weights = Decoder(self.target_vocab_size, self.pe_input, self.num_layers, self.d_model, self.num_heads, self.dff, self.dropout_rate)(tar, enc_output, training, look_ahead_mask, padding_mask)\n",
        "    final_output = layers.Dense(self.target_vocab_size)(dec_output)\n",
        "    return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oROP-pW8eIkI"
      },
      "outputs": [],
      "source": [
        "temp_input = tf.random.uniform((64, 62))\n",
        "temp_target = tf.random.uniform((64, 26))\n",
        "\n",
        "fn_out, _ = transformer(\n",
        "    temp_input, temp_target, training=False,\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500, target_vocab_size=8000, \n",
        "    pe_input=10000, pe_target=6000)\n",
        "fn_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gpkk2dLhVCSX"
      },
      "outputs": [],
      "source": [
        "temp_input = tf.random.uniform((64, 62))\n",
        "temp_target = tf.random.uniform((64, 26))\n",
        "\n",
        "fn_out, _ = Transformer(num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500, target_vocab_size=8000, \n",
        "    pe_input=10000, pe_target=6000)(temp_input, temp_target, training=False)\n",
        "fn_out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xy5v-C7ctKs_"
      },
      "source": [
        "**Depth Growing for NMT implementation** https://arxiv.org/abs/1907.01968\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQI5Xun1a2X6Z81uf3qUmQLVeP4Ls-uYIG8sBGiVSuVQSRQVm0kJg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "11XIdTjueekg"
      },
      "outputs": [],
      "source": [
        "class DepthGrowingTransformer(layers.Layer):\n",
        "  def __init__(\n",
        "      self, \n",
        "      input_vocab_size: int,\n",
        "      target_vocab_size: int,\n",
        "      pe_input: int,\n",
        "      pe_target: int,\n",
        "      n=6, \n",
        "      m=2, \n",
        "      d_model=1024, \n",
        "      dff=4096, \n",
        "      num_heads=16, \n",
        "      dropout_rate=0.3,\n",
        "  ):\n",
        "    super(DepthGrowingTransformer, self).__init__()\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.target_vocab_size = target_vocab_size\n",
        "    self.pe_input = pe_input\n",
        "    self.pe_target = pe_target\n",
        "    self.n = n\n",
        "    self.m = m\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "  def call(self, inp, tar, training, enc_mask=None, look_ahead_mask=None, padding_mask=None):\n",
        "    seq_len = tf.shape(inp)[1]\n",
        "    enc_out = layers.Embedding(self.input_vocab_size, self.d_model)(inp)\n",
        "    pos_encoding = positional_encoding(self.pe_input, self.d_model)\n",
        "    enc_out *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    enc_out += tf.slice(tf.cast(pos_encoding, tf.float32), [0, 0, 0], [pos_encoding.shape[0], seq_len, pos_encoding.shape[2]])\n",
        "    enc_out = layers.Dropout(self.dropout_rate)(enc_out, training)\n",
        "    enc_res = tf.identity(enc_out)\n",
        "    for i in range(self.n):\n",
        "      enc_out = EncoderLayer(self.d_model, self.num_heads, self.dff , self.dropout_rate)(enc_out, training, enc_mask)\n",
        "\n",
        "    seq_len = tf.shape(tar)[1]\n",
        "    dec_out = layers.Embedding(self.target_vocab_size, self.d_model)(tar)\n",
        "    pos_encoding = positional_encoding(self.pe_target, self.d_model)\n",
        "    dec_out *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    dec_out += tf.slice(tf.cast(pos_encoding, tf.float32), [0, 0, 0], [pos_encoding.shape[0], seq_len, pos_encoding.shape[2]])\n",
        "    dec_out = layers.Dropout(self.dropout_rate)(dec_out, training)\n",
        "    dec_res = tf.identity(dec_out)\n",
        "    attention_weights = {}\n",
        "    for i in range(self.n):\n",
        "      dec_out, block1, block2 = DecoderLayer(self.d_model, self.num_heads, self.dff, self.dropout_rate)(dec_out, tf.identity(enc_out), training, look_ahead_mask, padding_mask)\n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "    enc_out = enc_out + enc_res\n",
        "    dec_out = dec_out + dec_res\n",
        "    train_output = layers.Dense(self.target_vocab_size)(dec_out)\n",
        "\n",
        "    for i in range(self.m):\n",
        "      enc_out = EncoderLayer(self.d_model, self.num_heads, self.dff , self.dropout_rate)(enc_out, training, enc_mask)\n",
        "    for i in range(self.m):\n",
        "      dec_out, block3, block4 = DecoderLayer(self.d_model, self.num_heads, self.dff, self.dropout_rate)(dec_out, enc_out, training, look_ahead_mask, padding_mask)\n",
        "      attention_weights['decoder_layer{}_block3'.format(i+1)] = block3\n",
        "      attention_weights['decoder_layer{}_block4'.format(i+1)] = block4\n",
        "\n",
        "    final_output = layers.Dense(self.target_vocab_size)(dec_out)\n",
        "    return final_output, train_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C9-RYt0Lcv9b"
      },
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load(name=\"wmt14_translate/de-en\", with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qiXmAzsUM_OX"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for de, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_de = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (de.numpy() for de, en in train_examples), target_vocab_size=2**13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "O1LNo4ydNDzp"
      },
      "outputs": [],
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [tokenizer_de.vocab_size] + tokenizer_de.encode(\n",
        "      lang1.numpy()) + [tokenizer_de.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "  \n",
        "  return lang1, lang2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "USjzCnwFNRvE"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 40\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KGkDdtoDNcDt"
      },
      "outputs": [],
      "source": [
        "def tf_encode(pt, en):\n",
        "  return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HCmPs3UENdEr"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(\n",
        "    BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(filter_max_length).padded_batch(\n",
        "    BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "print(train_dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JbEGilqvpymS"
      },
      "outputs": [],
      "source": [
        "inp = K.Input(shape=(1,), name='en')\n",
        "tar = K.Input(shape=(1,), name='de')\n",
        "out = DepthGrowingTransformer( input_vocab_size=46675, target_vocab_size=63658, \n",
        "    pe_input=46675, pe_target=63658)(inp, tar, True)\n",
        "dgt = K.model(inputs=[inp, tar], outputs=out)\n",
        "optimizer = K.optimizers.Adam(learning_rate=1e-3)\n",
        "dgt.compile(optimizer, loss=K.losses.MeanSquaredError())\n",
        "dgt.fit(x_train, x_train, epochs=20, batch_size=64)\n",
        "print(fn_out)\n",
        "print(t_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e4fTocgClP_q"
      },
      "outputs": [],
      "source": [
        "temp_input = tf.random.uniform((64, 62))\n",
        "temp_target = tf.random.uniform((64, 26))\n",
        "\n",
        "fn_out, t_out, _ = DepthGrowingTransformer( input_vocab_size=8500, target_vocab_size=8000, \n",
        "    pe_input=10000, pe_target=6000)(temp_input, temp_target, training=False)\n",
        "print(fn_out)\n",
        "print(t_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "or2TNJO_lex9"
      },
      "outputs": [],
      "source": [
        "wmt_train = tfds.load(name=\"wmt14_translate/de-en\", split=\"train\")\n",
        "assert isinstance(wmt_train, tf.data.Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oNJaIaNWHOBK"
      },
      "outputs": [],
      "source": [
        "wmt_validation = tfds.load(name=\"wmt14_translate/de-en\", split=\"validation\")\n",
        "wmt_test = tfds.load(name=\"wmt14_translate/de-en\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HeaZu3cWGs8D"
      },
      "outputs": [],
      "source": [
        "de=\"/root/tensorflow_datasets/downloads/extracted/TAR_GZ.statmt.org_wmt14_training-parallel-nc-v9y4lT8pIpjmh3rkM8mJErknywdmswP0VAUS3dKGx0hIU.tgz/training/news-commentary-v9.de-en.de\"\n",
        "en=\"/root/tensorflow_datasets/downloads/extracted/TAR_GZ.statmt.org_wmt14_training-parallel-nc-v9y4lT8pIpjmh3rkM8mJErknywdmswP0VAUS3dKGx0hIU.tgz/training/news-commentary-v9.de-en.en\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DgNo8HgaE6Wn"
      },
      "outputs": [],
      "source": [
        "!subword-nmt learn-joint-bpe-and-vocab --input $de $en -s 89500 -o subwordBPE --write-vocabulary vocab.de vocab.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "T2BuYMdIKLOG"
      },
      "outputs": [],
      "source": [
        "!subword-nmt apply-bpe -c subwordBPE --vocabulary vocab.en --vocabulary-threshold 50 < {en} > train.BPE.en\n",
        "!subword-nmt apply-bpe -c subwordBPE --vocabulary vocab.de --vocabulary-threshold 50 < {de} > train.BPE.de"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SSBgT6YHEQ46"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}